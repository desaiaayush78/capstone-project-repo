{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Automated Fraud Risk Assessment Using News Analytics and NLP"
      ],
      "metadata": {
        "id": "VC_YQCgyH1sw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " ### Setup and Library Installation"
      ],
      "metadata": {
        "id": "nfmduZqVIXtC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install newsapi-python newspaper3k spacy fpdf lxml_html_clean\n",
        "!python -m spacy download en_core_web_sm\n",
        "\n",
        "from newsapi import NewsApiClient\n",
        "from newspaper import Article\n",
        "import spacy\n",
        "import pandas as pd\n",
        "from datetime import datetime, timedelta\n",
        "from fpdf import FPDF\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "from math import ceil\n",
        "\n",
        "# Initialization\n",
        "newsapi = NewsApiClient(api_key='8071ccec273940f89328374ebb540bd6')  # Replace with your actual API key\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n"
      ],
      "metadata": {
        "id": "7QB5YYWSIbA8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "94f20990-7dae-47f9-a72b-bf323014c051"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting newsapi-python\n",
            "  Downloading newsapi_python-0.2.7-py2.py3-none-any.whl.metadata (1.2 kB)\n",
            "Collecting newspaper3k\n",
            "  Downloading newspaper3k-0.2.8-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.11/dist-packages (3.8.5)\n",
            "Collecting fpdf\n",
            "  Downloading fpdf-1.7.2.tar.gz (39 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting lxml_html_clean\n",
            "  Downloading lxml_html_clean-0.4.2-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: requests<3.0.0 in /usr/local/lib/python3.11/dist-packages (from newsapi-python) (2.32.3)\n",
            "Requirement already satisfied: beautifulsoup4>=4.4.1 in /usr/local/lib/python3.11/dist-packages (from newspaper3k) (4.13.4)\n",
            "Requirement already satisfied: Pillow>=3.3.0 in /usr/local/lib/python3.11/dist-packages (from newspaper3k) (11.2.1)\n",
            "Requirement already satisfied: PyYAML>=3.11 in /usr/local/lib/python3.11/dist-packages (from newspaper3k) (6.0.2)\n",
            "Collecting cssselect>=0.9.2 (from newspaper3k)\n",
            "  Downloading cssselect-1.3.0-py3-none-any.whl.metadata (2.6 kB)\n",
            "Collecting lxml>=3.6.0 (from newspaper3k)\n",
            "  Downloading lxml-5.4.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: nltk>=3.2.1 in /usr/local/lib/python3.11/dist-packages (from newspaper3k) (3.9.1)\n",
            "Collecting feedparser>=5.2.1 (from newspaper3k)\n",
            "  Downloading feedparser-6.0.11-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting tldextract>=2.0.1 (from newspaper3k)\n",
            "  Downloading tldextract-5.3.0-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting feedfinder2>=0.0.4 (from newspaper3k)\n",
            "  Downloading feedfinder2-0.0.4.tar.gz (3.3 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting jieba3k>=0.35.1 (from newspaper3k)\n",
            "  Downloading jieba3k-0.35.1.zip (7.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m68.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.11/dist-packages (from newspaper3k) (2.9.0.post0)\n",
            "Collecting tinysegmenter==0.3 (from newspaper3k)\n",
            "  Downloading tinysegmenter-0.3.tar.gz (16 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.12)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (8.3.6)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.15.2)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (4.67.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.2)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.11.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy) (75.2.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (25.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.5.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4>=4.4.1->newspaper3k) (2.7)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4>=4.4.1->newspaper3k) (4.13.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from feedfinder2>=0.0.4->newspaper3k) (1.17.0)\n",
            "Collecting sgmllib3k (from feedparser>=5.2.1->newspaper3k)\n",
            "  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk>=3.2.1->newspaper3k) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk>=3.2.1->newspaper3k) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk>=3.2.1->newspaper3k) (2024.11.6)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.33.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0->newsapi-python) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0->newsapi-python) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0->newsapi-python) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0->newsapi-python) (2025.1.31)\n",
            "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.0)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
            "Collecting requests-file>=1.4 (from tldextract>=2.0.1->newspaper3k)\n",
            "  Downloading requests_file-2.1.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: filelock>=3.0.8 in /usr/local/lib/python3.11/dist-packages (from tldextract>=2.0.1->newspaper3k) (3.18.0)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (14.0.0)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.21.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.19.1)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
            "Downloading newsapi_python-0.2.7-py2.py3-none-any.whl (7.9 kB)\n",
            "Downloading newspaper3k-0.2.8-py3-none-any.whl (211 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.1/211.1 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lxml_html_clean-0.4.2-py3-none-any.whl (14 kB)\n",
            "Downloading cssselect-1.3.0-py3-none-any.whl (18 kB)\n",
            "Downloading feedparser-6.0.11-py3-none-any.whl (81 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.3/81.3 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lxml-5.4.0-cp311-cp311-manylinux_2_28_x86_64.whl (4.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m95.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tldextract-5.3.0-py3-none-any.whl (107 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m107.4/107.4 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading requests_file-2.1.0-py2.py3-none-any.whl (4.2 kB)\n",
            "Building wheels for collected packages: tinysegmenter, fpdf, feedfinder2, jieba3k, sgmllib3k\n",
            "  Building wheel for tinysegmenter (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tinysegmenter: filename=tinysegmenter-0.3-py3-none-any.whl size=13632 sha256=f9247ed543ab9bbaf01624e64b2c630a8a696d7bb56a98470cac47c48e616483\n",
            "  Stored in directory: /root/.cache/pip/wheels/fc/ab/f8/cce3a9ae6d828bd346be695f7ff54612cd22b7cbd7208d68f3\n",
            "  Building wheel for fpdf (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fpdf: filename=fpdf-1.7.2-py2.py3-none-any.whl size=40758 sha256=3bdaeb523cdcfa021ded084e420532c903ee74cb44d3d09c0a48145fdea1d821\n",
            "  Stored in directory: /root/.cache/pip/wheels/65/4f/66/bbda9866da446a72e206d6484cd97381cbc7859a7068541c36\n",
            "  Building wheel for feedfinder2 (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for feedfinder2: filename=feedfinder2-0.0.4-py3-none-any.whl size=3393 sha256=df4d918e9cced0b4e7ec1d41ef7db0da324e5deded47c734d723e1ee58507e58\n",
            "  Stored in directory: /root/.cache/pip/wheels/80/d5/72/9cd9eccc819636436c6a6e59c22a0fb1ec167beef141f56491\n",
            "  Building wheel for jieba3k (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for jieba3k: filename=jieba3k-0.35.1-py3-none-any.whl size=7398404 sha256=270eaab2d25724476359adafc2282daf0decda710e75c2f9a17dbbbb7096cf43\n",
            "  Stored in directory: /root/.cache/pip/wheels/3a/a1/46/8e68055c1713f9c4598774c15ad0541f26d5425ee7423b6493\n",
            "  Building wheel for sgmllib3k (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6089 sha256=d8f76ff102fc96a2905df824522e585899a945581edb6363fd9f22d6a0dbabfa\n",
            "  Stored in directory: /root/.cache/pip/wheels/3b/25/2a/105d6a15df6914f4d15047691c6c28f9052cc1173e40285d03\n",
            "Successfully built tinysegmenter fpdf feedfinder2 jieba3k sgmllib3k\n",
            "Installing collected packages: tinysegmenter, sgmllib3k, jieba3k, fpdf, lxml, feedparser, cssselect, requests-file, newsapi-python, lxml_html_clean, feedfinder2, tldextract, newspaper3k\n",
            "Successfully installed cssselect-1.3.0 feedfinder2-0.0.4 feedparser-6.0.11 fpdf-1.7.2 jieba3k-0.35.1 lxml-5.4.0 lxml_html_clean-0.4.2 newsapi-python-0.2.7 newspaper3k-0.2.8 requests-file-2.1.0 sgmllib3k-1.0.0 tinysegmenter-0.3 tldextract-5.3.0\n",
            "Collecting en-core-web-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m109.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load Dataset"
      ],
      "metadata": {
        "id": "hRKH6iTAIxRa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fraud_df = pd.read_csv(\"recent_fraud_cases_2023_2025.csv\")\n",
        "individuals = fraud_df[\"Name\"].tolist()\n"
      ],
      "metadata": {
        "id": "AnV2NO9lItF0"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Keyword Definition and Risk Scoring Functions"
      ],
      "metadata": {
        "id": "sO-0u5-vI1HM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "keyword_weights = {\n",
        "    \"fraud\": 3, \"scam\": 3, \"embezzlement\": 3, \"money laundering\": 3,\n",
        "    \"insider trading\": 3, \"ponzi scheme\": 3, \"wire fraud\": 3,\n",
        "    \"securities fraud\": 3, \"investment fraud\": 3, \"accounting fraud\": 3,\n",
        "    \"crypto fraud\": 3, \"financial misconduct\": 3, \"asset misappropriation\": 3,\n",
        "    \"misuse of funds\": 3, \"bank fraud\": 3, \"regulatory fraud\": 3,\n",
        "    \"indicted\": 2, \"charged\": 2, \"convicted\": 2, \"trial\": 2,\n",
        "    \"criminal complaint\": 2, \"prosecutors\": 2, \"federal charges\": 2,\n",
        "    \"regulatory violation\": 2, \"sec probe\": 2, \"doj\": 2,\n",
        "    \"market manipulation\": 2, \"false reporting\": 2,\n",
        "    \"misappropriation\": 2, \"whistleblower\": 1, \"audit failure\": 1,\n",
        "    \"investigation\": 1, \"court\": 1, \"lawsuit\": 1, \"breach\": 1,\n",
        "    \"kickbacks\": 2, \"forgery\": 2\n",
        "}\n",
        "\n",
        "risk_verbs = list(keyword_weights.keys())\n",
        "\n",
        "def calculate_score(text):\n",
        "    text = text.lower()\n",
        "    return sum(weight for kw, weight in keyword_weights.items() if kw in text)\n",
        "\n",
        "def get_risk_level(score):\n",
        "    return \"High\" if score >= 6 else \"Medium\" if score >= 3 else \"Low\"\n",
        "\n",
        "def get_flag(score):\n",
        "    return \"Escalate\" if score >= 6 else \"Review\" if score >= 3 else \"Monitor\"\n",
        "\n",
        "def get_matched_keywords(text):\n",
        "    text = text.lower()\n",
        "    return \", \".join([kw for kw in keyword_weights if kw in text])\n",
        "\n",
        "def clean(text):\n",
        "    return str(text).encode('latin-1', 'ignore').decode('latin-1')\n"
      ],
      "metadata": {
        "id": "-hQiQjNWI1jI"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### NLP-Based Article Filtering Functions"
      ],
      "metadata": {
        "id": "ABDBfOzbJAr8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def fetch_full_text(url):\n",
        "    try:\n",
        "        article = Article(url)\n",
        "        article.download()\n",
        "        article.parse()\n",
        "        return article.text\n",
        "    except:\n",
        "        return None\n",
        "\n",
        "def is_negative_about_person(article_text, person_name):\n",
        "    doc = nlp(article_text)\n",
        "    for sent in doc.sents:\n",
        "        if person_name.lower() in sent.text.lower():\n",
        "            if any(kw in sent.text.lower() for kw in risk_verbs):\n",
        "                return True\n",
        "    return False\n"
      ],
      "metadata": {
        "id": "-27HVzjWJBRE"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Bar Chart Generation Function"
      ],
      "metadata": {
        "id": "z58OmUbHJND6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_bar_chart(articles, person_name):\n",
        "    titles = [f\"Article {i+1}\" for i in range(len(articles))]\n",
        "    scores = [a[\"Negative News Score\"] for a in articles]\n",
        "    colors = ['#dc3545' if a[\"Risk Level\"] == \"High\" else '#ffc107' if a[\"Risk Level\"] == \"Medium\" else '#28a745' for a in articles]\n",
        "\n",
        "    plt.figure(figsize=(7, 4))\n",
        "    plt.bar(titles, scores, color=colors)\n",
        "    plt.xlabel(\"Articles\")\n",
        "    plt.ylabel(\"Negative News Score\")\n",
        "    plt.title(\"Risk Score per Article\")\n",
        "    plt.ylim(0, 10)\n",
        "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "\n",
        "    chart_path = f\"charts/{person_name.replace(' ', '_')}_bar_chart.png\"\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(chart_path)\n",
        "    plt.close()\n",
        "    return chart_path\n"
      ],
      "metadata": {
        "id": "wK7UEYWNJMj4"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### PDF Report Generation Class"
      ],
      "metadata": {
        "id": "Abq1_xA7Jboj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FraudReportPDF(FPDF):\n",
        "    def header(self):\n",
        "        self.set_font(\"Arial\", \"B\", 14)\n",
        "        self.set_text_color(0, 0, 128)\n",
        "        self.cell(0, 10, \"Financial Fraud Risk Report\", ln=True, align=\"C\")\n",
        "        self.ln(4)\n",
        "\n",
        "    def person_section(self, name, articles, chart_path):\n",
        "        self.set_font(\"Arial\", \"B\", 12)\n",
        "        self.set_text_color(0, 0, 0)\n",
        "        self.cell(0, 10, f\"Person: {clean(name)}\", ln=True)\n",
        "        self.ln(3)\n",
        "\n",
        "        if os.path.exists(chart_path):\n",
        "            self.image(chart_path, w=180)\n",
        "            self.ln(5)\n",
        "\n",
        "        for idx, article in enumerate(articles, 1):\n",
        "            self.set_font(\"Arial\", \"B\", 11)\n",
        "            self.multi_cell(0, 8, f\"{idx}. {clean(article['Title'])}\")\n",
        "            self.set_font(\"Arial\", \"\", 10)\n",
        "            self.cell(0, 6, f\"Date: {clean(article['Published At'])}\", ln=True)\n",
        "            self.cell(0, 6, f\"Negative News Score: {clean(str(article['Negative News Score']))}\", ln=True)\n",
        "            self.cell(0, 6, f\"Risk Level: {clean(article['Risk Level'])}\", ln=True)\n",
        "            self.cell(0, 6, f\"Flag: {clean(article['Flag'])}\", ln=True)\n",
        "            self.multi_cell(0, 6, f\"Matched Keywords: {clean(article['Matched Keywords'])}\")\n",
        "            self.set_text_color(0, 0, 255)\n",
        "            self.set_font(\"Arial\", \"I\", 10)\n",
        "            self.multi_cell(0, 6, f\"URL: {clean(article['URL'])}\")\n",
        "            self.set_text_color(0, 0, 0)\n",
        "            self.ln(4)\n"
      ],
      "metadata": {
        "id": "O65haQLmJakW"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Article Extraction and Report Generation"
      ],
      "metadata": {
        "id": "1AhoNOclJiQZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from_date = (datetime.now() - timedelta(days=30)).strftime('%Y-%m-%d')\n",
        "os.makedirs(\"fraud_news_pdfs\", exist_ok=True)\n",
        "os.makedirs(\"charts\", exist_ok=True)\n",
        "\n",
        "for name in individuals:\n",
        "    all_articles = []\n",
        "    keywords = list(keyword_weights.keys())\n",
        "    chunk_size = ceil(len(keywords) / 3)\n",
        "    keyword_chunks = [keywords[i:i + chunk_size] for i in range(0, len(keywords), chunk_size)]\n",
        "\n",
        "    for keyword_group in keyword_chunks:\n",
        "        query = f\"{name} AND ({' OR '.join(keyword_group)})\"\n",
        "        try:\n",
        "            results = newsapi.get_everything(\n",
        "                q=query,\n",
        "                language='en',\n",
        "                sort_by='relevancy',\n",
        "                from_param=from_date,\n",
        "                page_size=10\n",
        "            )\n",
        "            for article in results.get('articles', []):\n",
        "                full_text = fetch_full_text(article.get('url', ''))\n",
        "                if full_text and is_negative_about_person(full_text, name):\n",
        "                    score = calculate_score(full_text)\n",
        "                    all_articles.append({\n",
        "                        \"Title\": article.get(\"title\", \"\"),\n",
        "                        \"Published At\": article.get(\"publishedAt\", \"\")[:10],\n",
        "                        \"URL\": article.get(\"url\", \"\"),\n",
        "                        \"Negative News Score\": score,\n",
        "                        \"Risk Level\": get_risk_level(score),\n",
        "                        \"Flag\": get_flag(score),\n",
        "                        \"Matched Keywords\": get_matched_keywords(full_text)\n",
        "                    })\n",
        "        except Exception as e:\n",
        "            print(f\"Error for {name}: {e}\")\n",
        "\n",
        "    if all_articles:\n",
        "        chart_path = generate_bar_chart(all_articles, name)\n",
        "        pdf = FraudReportPDF()\n",
        "        pdf.add_page()\n",
        "        pdf.person_section(name, all_articles, chart_path)\n",
        "        pdf.output(f\"fraud_news_pdfs/{name.replace(' ', '_')}_report.pdf\")\n",
        "\n",
        "print(\"PDF reports generated in 'fraud_news_pdfs/' folder.\")\n"
      ],
      "metadata": {
        "id": "ILKy4C56Jivt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c80562b-9cf6-4ebd-f590-72e96ff590a4"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PDF reports generated in 'fraud_news_pdfs/' folder.\n"
          ]
        }
      ]
    }
  ]
}